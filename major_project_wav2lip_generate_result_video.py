# -*- coding: utf-8 -*-
"""Major_project_wav2lip_generate_result_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKV1k9wX1CEmDEKL9cEJVkNyZpdCEbLc

# Collab preliminaries
"""

!nvcc --version

from google.colab import drive   #mounting the local drive where files and datas are.
drive.mount('/content/gdrive')

"""# Get the code and models"""

!git clone https://github.com/Himanshu30122002/Wav2Lip.git    #cloning the trained model which we saved on github(personal).

!ls /content/gdrive/MyDrive/Wav2Lip

!cp -ri "/content/gdrive/MyDrive/Wav2Lip/wav2lip_gan.pth" /content/Wav2Lip/checkpoints/

"""# Get the pre-requisites"""

!pip uninstall tensorflow tensorflow-gpu    #uninstalling the outdated and not needed , we will install after this for version control.

!cd Wav2Lip && pip install -r requirements.txt     #in our wav2lip model(github) requirments.txt file is there from where we will install all dependencies needed

!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "Wav2Lip/face_detection/detection/sfd/s3fd.pth"

"""# Now lets try!"""

!cp "/content/gdrive/My Drive/Wav2Lip/Image.png" "/content/gdrive/My Drive/Wav2Lip/tamil_audio.wav" sample_data/     #copying input audio and video to collab sample data
!ls sample_data/

pip install librosa==0.9.0    #installing librosa new version as previous versions are deprected

pip install numba==0.58.1

!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "../sample_data/Image.png" --audio "../sample_data/tamil_audio.wav"
# here model takes the input audio and video/image creates a lip sync video

# use the "files" button on the left to download the result in the Wav2Lip/results/ folder.